#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ Ragas evaluation —Å –ª–æ–∫–∞–ª—å–Ω–æ–π Ollama-–º–æ–¥–µ–ª—å—é qwen3:8b.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –†–£–°–°–ö–ò–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò –∏–∑ russian_prompts.py –¥–ª—è –æ—Ü–µ–Ω–∫–∏ RAG-—Å–∏—Å—Ç–µ–º—ã.

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
  - ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ –∑–∞–ø—É—â–µ–Ω (https://ollama.com)
  - –º–æ–¥–µ–ª—å qwen3:8b –∑–∞–≥—Ä—É–∂–µ–Ω–∞: `ollama pull qwen3:8b`
  - Python-–ø–∞–∫–µ—Ç—ã: ragas, langchain-ollama, pyyaml, pydantic
  - sentence-transformers (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Å ragas)
  - –§–∞–π–ª russian_prompts.py –≤ —Ç–æ–º –∂–µ –∫–∞—Ç–∞–ª–æ–≥–µ (—Å–æ–¥–µ—Ä–∂–∏—Ç —Ä—É—Å—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã)

–ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏:
  1. answer_relevancy - —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ –≤–æ–ø—Ä–æ—Å—É
  2. faithfulness - –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å (–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π)
  3. context_precision - —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
  4. context_recall - –ø–æ–ª–Ω–æ—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

–ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞:
  ollama pull qwen3:8b
  python evaluate_rag_responses.py --input enriched_gold_questions_with_unfiltered_context.yaml --out results.json
  
  # –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä—É—Å—Å–∫–∏—Ö –ø—Ä–æ–º–ø—Ç–æ–≤:
  python evaluate_rag_responses.py --input data.yaml --out results.json --use-russian-templates

–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –°–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ä—É—Å—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã –∫ –º–µ—Ç—Ä–∏–∫–∞–º Ragas.
–ï—Å–ª–∏ russian_prompts.py –Ω–µ –Ω–∞–π–¥–µ–Ω, –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã.
"""
import argparse
import json
import logging
import sys
import yaml
from pathlib import Path
from ragas import evaluate
from ragas.dataset_schema import EvaluationDataset
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas.llms import LangchainLLMWrapper
from ragas.metrics import answer_relevancy, faithfulness, context_precision, context_recall

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä—É—Å—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã –∏ —à–∞–±–ª–æ–Ω—ã
try:
    from russian_prompts import (
        PROMPTS,
        PROMPT_EXAMPLES,
        ANSWER_RELEVANCY_TEMPLATE,
        FAITHFULNESS_TEMPLATE,
        CONTEXT_PRECISION_TEMPLATE,
        CONTEXT_RECALL_TEMPLATE,
    )

    RUSSIAN_PROMPTS_AVAILABLE = True
except ImportError:
    RUSSIAN_PROMPTS_AVAILABLE = False
    logging.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å russian_prompts.py - –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã")

# –û–ø–∏—Å–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫:
# - answer_relevancy (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞): –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –≤–æ–ø—Ä–æ—Å—É
# - faithfulness (–¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å): –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
# - context_precision (—Ç–æ—á–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞): –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤—ã—à–µ –≤ —Å–ø–∏—Å–∫–µ
# - context_recall (–ø–æ–ª–Ω–æ—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞): –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –≤–µ—Å—å –ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –±—ã–ª –∏–∑–≤–ª–µ—á–µ–Ω

# –í–ê–ñ–ù–û: –ö–∞–∫ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ —Ä—É—Å—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏—Å—å:
# 1. –ü—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–∫—Ä–∏–ø—Ç–∞ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—ã–≤–æ–¥ "üåç –ü–†–ò–ú–ï–ù–ï–ù–ò–ï –†–£–°–°–ö–ò–• –ü–†–û–ú–ü–¢–û–í –ö –ú–ï–¢–†–ò–ö–ê–ú RAGAS"
# 2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤–∏–¥–∏—Ç–µ "‚úÖ –†—É—Å—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã —Å few-shot –ø—Ä–∏–º–µ—Ä–∞–º–∏ —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã"
# 3. –í –ª–æ–≥–∞—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ LLM (üîµ LLM REQUEST) –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —è–∑—ã–∫ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π - –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º
# 4. –ï—Å–ª–∏ –≤–∏–¥–∏—Ç–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ:
#    - –ù–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ russian_prompts.py –≤ –∫–∞—Ç–∞–ª–æ–≥–µ task_7_evaluation/
#    - –ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∏–º–ø–æ—Ä—Ç–∞ (–Ω–µ—Ç –æ—à–∏–±–æ–∫ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ)
#    - –°—Ç—Ä—É–∫—Ç—É—Ä—É –º–µ—Ç—Ä–∏–∫ Ragas (–º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –≤ —Ä–∞–∑–Ω—ã—Ö –≤–µ—Ä—Å–∏—è—Ö)

# LangChain imports
try:
    from langchain_ollama import ChatOllama
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.callbacks import BaseCallbackHandler
    from langchain_core.outputs import LLMResult
    from sentence_transformers import SentenceTransformer
except Exception as e:
    print(f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
    print("–¢—Ä–µ–±—É—é—Ç—Å—è –ø–∞–∫–µ—Ç—ã: pip install langchain-ollama langchain-huggingface")
    sys.exit(1)


class LLMLoggingCallback(BaseCallbackHandler):
    """Callback –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ LLM –∏ –æ—Ç–≤–µ—Ç–æ–≤"""

    def __init__(self):
        self.request_count = 0

    def on_llm_start(self, serialized, prompts, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –Ω–∞—á–∞–ª–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM"""
        self.request_count += 1
        print(f"\n{'='*80}")
        print(f"üîµ LLM REQUEST #{self.request_count}")
        print(f"{'='*80}")
        for i, prompt in enumerate(prompts, 1):
            print(f"\n--- –ü—Ä–æ–º–ø—Ç {i}/{len(prompts)} ---")
            print(prompt)
            print(f"--- –ö–æ–Ω–µ—Ü –ø—Ä–æ–º–ø—Ç–∞ {i} ---")

    def on_llm_end(self, response: LLMResult, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM"""
        print(f"\n{'='*80}")
        print(f"üü¢ LLM RESPONSE #{self.request_count}")
        print(f"{'='*80}")
        for i, generation in enumerate(response.generations, 1):
            for j, gen in enumerate(generation, 1):
                print(f"\n--- –û—Ç–≤–µ—Ç {i}.{j} ---")
                print(gen.text)
                print(f"--- –ö–æ–Ω–µ—Ü –æ—Ç–≤–µ—Ç–∞ {i}.{j} ---")
        print(f"{'='*80}\n")


def apply_russian_prompts_to_metrics():
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä—É—Å—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã –∫ –º–µ—Ç—Ä–∏–∫–∞–º Ragas —á–µ—Ä–µ–∑ –º–µ—Ç–æ–¥ set_prompts().

    –í Ragas –∫–∞–∂–¥–∞—è –º–µ—Ç—Ä–∏–∫–∞ –∏–º–µ–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–æ–º–ø—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å.
    –ú—ã –∑–∞–º–µ–Ω—è–µ–º –∏—Ö –Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ —Ä—É—Å—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã —Å few-shot –ø—Ä–∏–º–µ—Ä–∞–º–∏.
    """
    if not RUSSIAN_PROMPTS_AVAILABLE:
        logging.warning("–†—É—Å—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ")
        return False

    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Prompt –æ–±—ä–µ–∫—Ç–æ–≤
        if PROMPTS is None:
            logging.warning("Prompt –æ–±—ä–µ–∫—Ç—ã –Ω–µ —Å–æ–∑–¥–∞–Ω—ã –≤ russian_prompts.py")
            return False

        logging.info("–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ä—É—Å—Å–∫–∏—Ö Prompt –æ–±—ä–µ–∫—Ç–æ–≤ –∫ –º–µ—Ç—Ä–∏–∫–∞–º...")

        # –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Å –∏—Ö –ø—Ä–æ–º–ø—Ç–∞–º–∏ –∏ –∫–ª—é—á–∞–º–∏
        # –ö–∞–∂–¥–∞—è –º–µ—Ç—Ä–∏–∫–∞ –∏–º–µ–µ—Ç —Å–≤–æ–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∫–ª—é—á–∏ –ø—Ä–æ–º–ø—Ç–æ–≤
        prompt_mappings = {
            "answer_relevancy": {
                "metric": answer_relevancy,
                "prompt_key": "response_relevance_prompt",  # –ö–ª—é—á –≤ set_prompts()
                "russian_prompt": PROMPTS["answer_relevancy"],
            },
            "faithfulness": {
                "metric": faithfulness,
                # –£ faithfulness 2 –ø—Ä–æ–º–ø—Ç–∞, –∑–∞–º–µ–Ω—è–µ–º statement_generator
                "prompt_key": "statement_generator_prompt",
                "russian_prompt": PROMPTS["faithfulness"],
            },
            "context_precision": {
                "metric": context_precision,
                "prompt_key": "context_precision_prompt",
                "russian_prompt": PROMPTS["context_precision"],
            },
            "context_recall": {
                "metric": context_recall,
                "prompt_key": "context_recall_classification_prompt",
                "russian_prompt": PROMPTS["context_recall"],
            },
        }

        applied_count = 0
        for metric_name, config in prompt_mappings.items():
            try:
                metric_obj = config["metric"]
                prompt_key = config["prompt_key"]
                russian_prompt = config["russian_prompt"]

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º set_prompts(**{key: prompt})
                metric_obj.set_prompts(**{prompt_key: russian_prompt})

                logging.info(f"  ‚úì {metric_name}: –ø—Ä–æ–º–ø—Ç '{prompt_key}' –∑–∞–º–µ–Ω—ë–Ω –Ω–∞ —Ä—É—Å—Å–∫–∏–π")
                applied_count += 1

            except Exception as e:
                logging.error(f"  ‚úó {metric_name}: –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–º–µ–Ω–µ –ø—Ä–æ–º–ø—Ç–∞: {e}")
                continue

        if applied_count > 0:
            logging.info(f"‚úÖ –ü—Ä–∏–º–µ–Ω–µ–Ω–æ {applied_count} —Ä—É—Å—Å–∫–∏—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –∫ –º–µ—Ç—Ä–∏–∫–∞–º")
            return True
        else:
            logging.warning("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞")
            return False

    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ —Ä—É—Å—Å–∫–∏—Ö –ø—Ä–æ–º–ø—Ç–æ–≤: {e}", exc_info=True)
        return False


def load_dataset_yaml(path: str):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ YAML.

    –û–∂–∏–¥–∞–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (—Å–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π):
    - question: "–≤–æ–ø—Ä–æ—Å1"
      expected_answer: "—ç—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç"
      context: ["–∫–æ–Ω—Ç–µ–∫—Å—Ç1", "–∫–æ–Ω—Ç–µ–∫—Å—Ç2", ...]
      answer: "–æ—Ç–≤–µ—Ç RAG-—Å–∏—Å—Ç–µ–º—ã"
    - question: "–≤–æ–ø—Ä–æ—Å2"
      ...

    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Ragas EvaluationDataset:
    - user_input: –≤–æ–ø—Ä–æ—Å
    - response: –æ—Ç–≤–µ—Ç RAG-—Å–∏—Å—Ç–µ–º—ã
    - reference: —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
    - retrieved_contexts: —Å–ø–∏—Å–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
    """
    with open(path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)

    # data –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º –∑–∞–ø–∏—Å–µ–π
    if not isinstance(data, list):
        raise ValueError(f"–û–∂–∏–¥–∞–µ—Ç—Å—è —Å–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π –≤ YAML, –ø–æ–ª—É—á–µ–Ω {type(data)}")

    examples = []
    for i, item in enumerate(data):
        if not isinstance(item, dict):
            logging.warning(f"–ó–∞–ø–∏—Å—å {i} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue

        question = item.get("question", "")
        expected_answer = item.get("expected_answer", "")
        context = item.get("context", [])
        answer = item.get("answer", "")

        if not question:
            logging.warning(f"–ó–∞–ø–∏—Å—å {i} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–æ–ø—Ä–æ—Å–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue

        example = {
            "user_input": question,
            "response": answer,
            "reference": expected_answer,
            "retrieved_contexts": context,
        }

        examples.append(example)

    logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(examples)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {path}")
    return examples


def main():
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Å–∞–º–æ–º –Ω–∞—á–∞–ª–µ
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--input", "-i", required=True, help="Path to dataset YAML file (e.g., enriched_gold_questions.yaml)"
    )
    parser.add_argument("--out", "-o", default="ragas_results_ollama.json", help="Output path for results")
    parser.add_argument("--model", default="qwen3:8b", help="Ollama model name (default qwen3:8b)")
    parser.add_argument(
        "--embedding_model",
        default="Qwen/Qwen3-Embedding-4B",
        help="HuggingFace embedding model (default: Qwen3-Embedding-4B)",
    )
    parser.add_argument("--batch_size", type=int, default=8)
    parser.add_argument(
        "--use-russian-templates",
        action="store_true",
        help="Use Russian templates for evaluation (default: False)"
    )
    args = parser.parse_args()

    samples = load_dataset_yaml(args.input)
    samples = [s for s in samples if s["user_input"] and s["response"]]
    if not samples:
        print("–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –≤—Ö–æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ.")
        return

    eval_dataset = EvaluationDataset.from_list(samples)

    # –°–æ–∑–¥–∞—ë–º –ª–æ–∫–∞–ª—å–Ω—ã–µ embeddings (HuggingFace) –Ω–∞ CPU
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ embedding –º–æ–¥–µ–ª–∏ –Ω–∞ CPU: {args.embedding_model}...")

    # –î–ª—è Qwen3-Embedding –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —á—Ç–æ –∏ –≤ extract_embeddings_for_kb.py
    # HuggingFaceEmbeddings –ø—Ä–∏–Ω–∏–º–∞–µ—Ç model_name (—Å—Ç—Ä–æ–∫—É) –∏ model_kwargs
    if "Qwen3-Embedding" in args.embedding_model:
        langchain_embeddings = HuggingFaceEmbeddings(
            model_name=args.embedding_model,
            model_kwargs={"device": "cpu", "tokenizer_kwargs": {"padding_side": "left"}},
            encode_kwargs={"normalize_embeddings": True},
        )
    else:
        langchain_embeddings = HuggingFaceEmbeddings(
            model_name=args.embedding_model,
            model_kwargs={"device": "cpu"},
            encode_kwargs={"normalize_embeddings": True},
        )

    # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –¥–ª—è ragas (—Å –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ–º deprecation warning)
    import warnings

    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=DeprecationWarning)
        ragas_embeddings = LangchainEmbeddingsWrapper(langchain_embeddings)

    # –°–æ–∑–¥–∞—ë–º ChatOllama –∏–∑ langchain-ollama (–Ω–æ–≤–∞—è –≤–µ—Ä—Å–∏—è)
    print(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama-–º–æ–¥–µ–ª–∏: {args.model}...")

    # –°–æ–∑–¥–∞—ë–º callback –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤
    llm_callback = LLMLoggingCallback()

    # –í–∫–ª—é—á–∞–µ–º verbose –∏ –¥–æ–±–∞–≤–ª—è–µ–º callback –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è,
    # reasoning —É–±–∏—Ä–∞–µ–º, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ <think-—Å–µ–∫—Ü–∏–∏> –≤ –æ—Ç–≤–µ—Ç–µ
    chat_ollama = ChatOllama(model=args.model, temperature=0.0, verbose=True, reasoning=False, callbacks=[llm_callback])
    llm_wrapper = LangchainLLMWrapper(chat_ollama)

    print("–ó–∞–ø—É—Å–∫–∞—é evaluate() —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ ‚Äî —ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏...")
    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: batch_size={args.batch_size} (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á –æ—Ü–µ–Ω–∫–∏, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)")

    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä—É—Å—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã –∫ –º–µ—Ç—Ä–∏–∫–∞–º, –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ
    if args.use_russian_templates:
        print("\n" + "=" * 80)
        print("üåç –ü–†–ò–ú–ï–ù–ï–ù–ò–ï –†–£–°–°–ö–ò–• –ü–†–û–ú–ü–¢–û–í –ö –ú–ï–¢–†–ò–ö–ê–ú RAGAS")
        print("=" * 80)

        if RUSSIAN_PROMPTS_AVAILABLE:
            print(f"üìÑ –§–∞–π–ª russian_prompts.py –Ω–∞–π–¥–µ–Ω –∏ –∑–∞–≥—Ä—É–∂–µ–Ω")
            print(f"üì¶ PydanticPrompt –æ–±—ä–µ–∫—Ç—ã: {'‚úì –î–æ—Å—Ç—É–ø–Ω—ã' if PROMPTS else '‚úó –ù–µ–¥–æ—Å—Ç—É–ø–Ω—ã'}")
            
            russian_prompts_applied = apply_russian_prompts_to_metrics()
            
            if russian_prompts_applied:
                print("\n‚úÖ –†—É—Å—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã —Å few-shot –ø—Ä–∏–º–µ—Ä–∞–º–∏ —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã –∫–æ –≤—Å–µ–º –º–µ—Ç—Ä–∏–∫–∞–º!")
                print("   –ú–æ–¥–µ–ª—å-–∫—Ä–∏—Ç–∏–∫ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –ø—Ä–∏–º–µ—Ä—ã.")
            else:
                print("\n‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å —Ä—É—Å—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã Ragas.")
        else:
            print("‚ö†Ô∏è  –§–∞–π–ª russian_prompts.py –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã Ragas.")
            print("   –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä—É—Å—Å–∫–∏—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª russian_prompts.py –¥–æ—Å—Ç—É–ø–µ–Ω.")
            
        print("=" * 80 + "\n")
    else:
        print("\n‚ÑπÔ∏è  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä—É—Å—Å–∫–∏—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –æ—Ç–∫–ª—é—á–µ–Ω–æ. –ü—Ä–∏–º–µ–Ω—è—é—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã Ragas.")
        print("   –î–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–ª–∞–≥ --use-russian-templates\n")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    # –¢–∞–∫ –∫–∞–∫ –≤ YAML –µ—Å—Ç—å –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–æ–ª—è (retrieved_contexts, reference), –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏
    metrics = [
        answer_relevancy,  # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞: –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å
        faithfulness,  # –î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å: –æ—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω —Ç–æ–ª—å–∫–æ –Ω–∞ —Ñ–∞–∫—Ç–∞—Ö –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–±–µ–∑ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π)
        context_precision,  # –¢–æ—á–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ç–æ—á–Ω–æ RAG —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        context_recall,  # –ü–æ–ª–Ω–æ—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: –≤—Å–µ –ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑–≤–ª–µ—á–µ–Ω—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    ]

    # –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –¥–∞–Ω–Ω—ã–º –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏:
    # - answer_relevancy: user_input, response
    # - faithfulness: response, retrieved_contexts
    # - context_precision: user_input, retrieved_contexts, reference
    # - context_recall: retrieved_contexts, reference

    print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ ({len(metrics)} –º–µ—Ç—Ä–∏–∫):")
    for i, metric in enumerate(metrics, 1):
        print(f"  {i}. {metric.name}: {metric.__doc__.split('.')[0] if metric.__doc__ else 'N/A'}")

    total_tasks = len(samples) * len(metrics)
    num_batches = (total_tasks + args.batch_size - 1) // args.batch_size
    print(f"\nüî¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á: {len(samples)} –ø—Ä–∏–º–µ—Ä–æ–≤ √ó {len(metrics)} –º–µ—Ç—Ä–∏–∫ = {total_tasks} –∑–∞–¥–∞—á")
    print(f"   –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤ {num_batches} –±–∞—Ç—á–∞—Ö –ø–æ {args.batch_size} –∑–∞–¥–∞—á\n")

    result = evaluate(
        eval_dataset,
        llm=llm_wrapper,
        embeddings=ragas_embeddings,
        metrics=metrics,  # –Ø–≤–Ω–æ –ø–µ—Ä–µ–¥–∞—ë–º –º–µ—Ç—Ä–∏–∫–∏
        show_progress=True,
        batch_size=args.batch_size,
    )

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è JSON-—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    print(f"\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:")
    result_dict = result.to_pandas().to_dict("records")

    # –¢–∞–∫–∂–µ –≤—ã–≤–æ–¥–∏–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
    print(f"  –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫:")
    for metric_name in result.to_pandas().columns:
        if metric_name not in ["user_input", "response", "retrieved_contexts", "reference"]:
            mean_value = result.to_pandas()[metric_name].mean()
            print(f"    {metric_name}: {mean_value:.4f}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON
    with open(args.out, "w", encoding="utf-8") as f:
        json.dump(result_dict, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {args.out}")


if __name__ == "__main__":
    main()
