# Задание 7. Аналитика покрытия и качества базы знаний

Здесь я ~~пародировал~~ выполнял оценку качества RAG-системы с помощью RAGAS.

Я считал 4 стандартных метрики:
  1. answer_relevancy - релевантность ответа вопросу
  2. faithfulness - достоверность (отсутствие галлюцинаций)
  3. context_precision - точность ранжирования контекста
  4. context_recall - полнота извлечения контекста

Оценка выполнялась в офлайн-режиме (т.е. сначала генерируем датасет ответов, потом считаем метрики).

Использовалась модель-критик, в качестве критика поставил такой же `qwen3:8b` с 4-битовой квантизацией: на удивление, он уже мог генерировать ответы в том формате, в котором RAGAS ему заказывал = )

По уму -- в качестве критика стоит взять модель посерьёзнее (например, `Яндекс-GPT` или `qwen3-235B` на облачном сервисе того же "Яндекса"), но сроки сдачи уже поджимали, и возиться с ними я не стал.

## Структура файлов

### Скрипты генерации и оценки

- **[`generate_rag_responses.py`](generate_rag_responses.py)** — скрипт для генерации ответов RAG-системы на золотые вопросы.  
  Загружает вопросы из `gold_questions.yaml`, получает ответы и контекст от RAG, сохраняет обогащённые данные в YAML-файлы.

- **[`evaluate_rag_responses.py`](evaluate_rag_responses.py)** — скрипт для оценки RAG-системы с помощью RAGAS метрик.  
  Использует локальную модель `qwen3:8b` (через Ollama) в качестве критика, поддерживает русские промпты для метрик.

### Датасеты

- **[`gold_questions.yaml`](gold_questions.yaml)** — набор из 13 золотых вопросов с ожидаемыми ответами (ground truth).  
  Включает как вопросы по базе знаний (7 шт.), так и вопросы-ловушки для проверки галлюцинаций (6 шт.).

- **[`enriched_gold_questions.yaml`](enriched_gold_questions.yaml)** — обогащённый датасет на русском языке.  
  Содержит: вопрос, ожидаемый ответ, **отфильтрованный** контекст (топ-4 чанка) и ответ RAG-системы.

- **[`enriched_gold_questions_with_unfiltered_context.yaml`](enriched_gold_questions_with_unfiltered_context.yaml)** — обогащённый датасет на русском с **неотфильтрованным** контекстом.  
  Контекст включает все чанки до применения фильтра допустимого контента.

- **[`enriched_gold_questions_with_unfiltered_context_en.yaml`](enriched_gold_questions_with_unfiltered_context_en.yaml)** — англоязычная версия датасета с неотфильтрованным контекстом.  
  Использовалась для проверки работы RAGAS (т.к. оказалось, что если промпты и примеры к критику даются на английском, а контекст и ответы в моём датасете на русском, то моя слабенькая модель-критик будет справляться хуже, и оценка RAGAS будет занижена)

### Результаты оценки

- **[`ragas_results_ollama_ru.json`](ragas_results_ollama_ru.json)** — результаты оценки RAGAS на русском датасете.  
  Содержит значения 4 метрик для каждого из 13 вопросов.  
  **Средние значения метрик (RU):**
  - answer_relevancy: 0.4175
  - faithfulness: 0.8718
  - context_precision: 0.6496
  - context_recall: 0.8846

- **[`ragas_results_ollama_en.json`](ragas_results_ollama_en.json)** — результаты оценки RAGAS на датасете, переведённом на английский.  
  **Средние значения метрик (EN):**
  - answer_relevancy: 0.4930
  - faithfulness: 0.9487
  - context_precision: 0.4551
  - context_recall: 0.9615

### Логи оценки

- **[`answer_evaluation_ru.log`](answer_evaluation_ru.log)** — полный лог процесса оценки на русском датасете (8300+ строк).  
  Содержит все промпты, отправленные к модели-критику, и её ответы.

- **[`answer_evaluation_en.log`](answer_evaluation_en.log)** — полный лог процесса оценки на английском датасете (8312+ строк).  
  Показывает, как RAGAS взаимодействует с `qwen3:8b` через Ollama.

### Зачем я переводил датасет на английский?
Что интересно, если промпты и примеры к критику RAGAS даёт на английском, а контекст и ответы в моём датасете на русском, то моя слабенькая модель-критик будет справляться хуже, и оценка RAGAS будет занижена.

Поэтому я сделал читерский трюк: перевёл датасет-для-оценки на английскй с помощью автоперевода в ChatGPT, чтобы язык оцениваемого датасета совпадал с языком промптов и примеров для RAGAS-критика.

Автоматическую адаптацию промптов RAGAS я пробовал, мне не понравилось (возможно, надо делать её на LLM помощнее, но не проверял).

Есть ещё вариант -- переработать шаблоны промптов RAGAS, чтобы он отправлял запросы на русском.

Примеры можно найти тут: https://github.com/ai-forever/gigaragas/commits/main/

Но: это всё требовало времени (хотя бы на то, чтобы разучить сематнику RAGAS-шаблонов), а трививальное решение оказалось вполне жизнеспособным.

![img.png](img.png)