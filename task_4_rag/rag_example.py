#!/usr/bin/env python3
"""RAG —á–∞—Ç-–±–æ—Ç —Å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –ø–æ–∏—Å–∫–∞ –∏ RetrievalQAWithSourcesChain"""

import logging
import re
import sys
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from langchain_ollama import OllamaLLM, ChatOllama
from langchain.chains import RetrievalQAWithSourcesChain, LLMChain
from langchain.schema import Document
from langchain.prompts import (
    PromptTemplate,
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    AIMessagePromptTemplate,
)
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks import CallbackManagerForRetrieverRun, BaseCallbackHandler
from langchain_core.outputs import LLMResult

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

logger = logging.getLogger(__name__)


class PromptLoggingCallback(BaseCallbackHandler):
    """Callback –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º—ã—Ö –≤ LLM"""

    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –Ω–∞—á–∞–ª–µ —Ä–∞–±–æ—Ç—ã LLM"""
        logger.info("=" * 80)
        logger.info("–ü–†–û–ú–ü–¢, –û–¢–ü–†–ê–í–õ–Ø–ï–ú–´–ô –í LLM:")
        logger.info("=" * 80)
        for i, prompt in enumerate(prompts, 1):
            logger.info(f"\n--- –ü—Ä–æ–º–ø—Ç #{i} ---")
            logger.info(prompt)
        logger.info("=" * 80)

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ä–∞–±–æ—Ç—ã LLM"""
        logger.info("-" * 80)
        logger.info("–û–¢–í–ï–¢ –û–¢ LLM:")
        logger.info("-" * 80)
        for generation in response.generations:
            for gen in generation:
                logger.info(gen.text)
        logger.info("=" * 80)


# –°—É—â–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ —É–±–∏—Ä–∞–µ–º –∏–∑ –ø–æ–∏—Å–∫–æ–≤–æ–π –≤—ã–¥–∞—á–∏
FORBIDDEN_ENTITIES = ["—Å—É–ø–µ—Ä–ø–∞—Ä–æ–ª", "superpassword"]


class CustomChromaRetriever(BaseRetriever):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏–µ–π"""

    suffix: str = "4B-8bit"  # –ú–æ–¥–µ–ª—å–Ω—ã–π —Å—É—Ñ—Ñ–∏–∫—Å: "0.6B" –∏–ª–∏ "4B"
    collection_name: str = "kb_embeddings"
    k: int = 4  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    use_cpu: bool = False  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CPU –¥–ª—è embeddings
    use_8bit: bool = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é 8 –±–∏—Ç
    use_chunk_filtering: bool = False  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é —á–∞–Ω–∫–æ–≤ –æ—Ç –æ–ø–∞—Å–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π

    def __init__(
        self,
        suffix: str = "4B",
        use_cpu: bool = True,
        use_8bit: bool = True,
        use_chunk_filtering: bool = True,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.suffix = suffix
        self.use_cpu = use_cpu
        self.use_chunk_filtering = use_chunk_filtering

        # –ü—É—Ç—å –∫ ChromaDB —Å —É—á—ë—Ç–æ–º —Å—É—Ñ—Ñ–∏–∫—Å–∞
        chroma_db_path = Path(__file__).parent.parent / "task_3_vector_index" / "chroma" / f"chroma-{suffix}"

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB –∫–ª–∏–µ–Ω—Ç–∞
        self._chroma_client = chromadb.PersistentClient(
            path=str(chroma_db_path), settings=Settings(anonymized_telemetry=False)
        )
        # –ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é
        self._collection = self._chroma_client.get_collection(name=self.collection_name)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è embeddings –º–æ–¥–µ–ª–∏ (Qwen3-Embedding)
        self._model = SentenceTransformer(
            f"Qwen/Qwen3-Embedding-{suffix}",
            device="cpu" if use_cpu else "cuda",
            model_kwargs={"load_in_8bit": use_8bit},
            tokenizer_kwargs={"padding_side": "left"},
        )

    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun = None
    ) -> List[Document]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ ChromaDB –ø–æ –∑–∞–ø—Ä–æ—Å—É.

        Args:
            query: —Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞
            run_manager: callback manager (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

        Returns:
            —Å–ø–∏—Å–æ–∫ Document —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (source, token_range, char_range)
        """
        # –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏

        # –ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ (—Ç–∞–∫ –∂–µ, –∫–∞–∫ –≤ extract_embeddings_for_kb.py)
        query_embedding = self._model.encode(query).tolist()

        # –ü–æ–∏—Å–∫ –≤ ChromaDB
        results = self._collection.query(query_embeddings=[query_embedding], n_results=self.k)

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Document –æ–±—ä–µ–∫—Ç—ã —Å –Ω—É–º–µ—Ä–∞—Ü–∏–µ–π
        documents = []
        if results["metadatas"] and len(results["metadatas"]) > 0:
            doc_index = 1
            for metadata in results["metadatas"][0]:
                text = metadata["text"]

                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏
                if any(forbidden in text.lower() for forbidden in FORBIDDEN_ENTITIES):
                    logger.info(f"üîí –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∑–∞–ø—Ä–µ—â—ë–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π: {FORBIDDEN_ENTITIES}")
                    logger.info(metadata["source_path"])
                    logger.info("Document filtered out!")
                    continue

                # –î–æ–±–∞–≤–ª—è–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é –ø—Ä—è–º–æ –≤ page_content
                doc = Document(
                    page_content=f"[{doc_index}] {text}",
                    metadata={
                        "source": metadata["source_path"],
                        "token_range": (metadata["token_range_start"], metadata["token_range_end"]),
                        "char_range": (metadata["char_range_start"], metadata["char_range_end"]),
                    },
                )
                documents.append(doc)
                doc_index += 1

        return documents


class StubRetriever(BaseRetriever):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –ø–æ–∏—Å–∫–∞"""

    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun = None
    ) -> List[Document]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É.
        –ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ–∏—Å–∫–∞.

        Args:
            query: —Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞
            run_manager: callback manager (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

        Returns:
            —Å–ø–∏—Å–æ–∫ Document —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (source, token_range, char_range)
        """
        # –ü—Ä–∏–º–µ—Ä –∑–∞–≥–ª—É—à–∫–∏ - –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –≤–∞—à—É —Ä–µ–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ–∏—Å–∫–∞
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –≤–∞—à–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π

        documents = [
            Document(
                page_content="[1] SQLite - —ç—Ç–æ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è SQL –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –Ω–µ —Ç—Ä–µ–±—É—é—â–∞—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞.",
                metadata={
                    "source": "https://example.com/docs/sqlite_intro.html",
                    "token_range": (0, 15),
                    "char_range": (0, 85),
                },
            ),
            Document(
                page_content="[2] SQLite —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö –Ω–∞ iOS –∏ Android.",
                metadata={
                    "source": "https://example.com/docs/sqlite_mobile.html",
                    "token_range": (120, 135),
                    "char_range": (580, 650),
                },
            ),
            Document(
                page_content="[3] –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö SQLite —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ –Ω–∞ –¥–∏—Å–∫–µ.",
                metadata={
                    "source": "/path/to/local/sqlite_storage.txt",
                    "token_range": (45, 58),
                    "char_range": (220, 273),
                },
            ),
            Document(
                page_content="[4] SQLite –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ ACID –∏ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö SQL –æ–ø–µ—Ä–∞—Ü–∏–π.",
                metadata={
                    "source": "https://example.com/docs/sqlite_features.html",
                    "token_range": (200, 218),
                    "char_range": (1050, 1128),
                },
            ),
        ]

        return documents


def load_prompt_template(template_path: Path = Path(__file__).parent / "prompt_template.txt") -> str:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —à–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
    with open(template_path, "r", encoding="utf-8") as f:
        return f.read()


def load_ginecarum_prompts() -> Dict[str, str]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ —Å–µ–∫—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–∞ Ginecarum –∏–∑ –ø–∞–ø–∫–∏"""
    prompt_dir = Path(__file__).parent / "ginecarum_prompt"

    sections = {
        "system": "system.txt",
        "example1_user": "example1_user.txt",
        "example1_assistant": "example1_assistant.txt",
        "example2_user": "example2_user.txt",
        "example2_assistant": "example2_assistant.txt",
        "query_user": "query_user.txt",
    }

    prompts = {}
    for key, filename in sections.items():
        file_path = prompt_dir / filename
        with open(file_path, "r", encoding="utf-8") as f:
            prompts[key] = f.read()

    return prompts


def create_rag_chain(use_chunk_filtering: bool = True) -> RetrievalQAWithSourcesChain:
    """–°–æ–∑–¥–∞—ë—Ç RAG chain —Å Qwen 3 –∏ –∫–∞—Å—Ç–æ–º–Ω—ã–º —Ä–µ—Ç—Ä–∏–≤–µ—Ä–æ–º"""

    # –°–æ–∑–¥–∞—Ç—å callback –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤
    prompt_callback = PromptLoggingCallback()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChatOllama (–≤–º–µ—Å—Ç–æ OllamaLLM) —Å callback
    llm = ChatOllama(model="qwen3:8b", temperature=0.5, callbacks=[prompt_callback])

    # –°–æ–∑–¥–∞—Ç—å –∫–∞—Å—Ç–æ–º–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä
    # retriever = StubRetriever()  # –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ ChromaDB

    retriever = CustomChromaRetriever(
        suffix="4B", use_cpu=False, use_8bit=True, use_chunk_filtering=use_chunk_filtering
    )  # –†–µ–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ ChromaDB

    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–º–ø—Ç-—Ç–µ–º–ø–ª–µ–π—Ç—ã –∏–∑ —Ñ–∞–π–ª–æ–≤
    prompts = load_ginecarum_prompts()

    # –°–æ–∑–¥–∞—Ç—å ChatPromptTemplate —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–æ–ª—è–º–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–µ–∫—Ü–∏–π
    combine_prompt = ChatPromptTemplate.from_messages(
        [
            # System message: –æ—Å–Ω–æ–≤–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            SystemMessagePromptTemplate.from_template(prompts["system"]),
            # Few-shot Example 1 - User message
            HumanMessagePromptTemplate.from_template(prompts["example1_user"]),
            # Few-shot Example 1 - Assistant response
            AIMessagePromptTemplate.from_template(prompts["example1_assistant"]),
            # Few-shot Example 2 - User message
            HumanMessagePromptTemplate.from_template(prompts["example2_user"]),
            # Few-shot Example 2 - Assistant response
            AIMessagePromptTemplate.from_template(prompts["example2_assistant"]),
            # Actual query - User message with context and question
            HumanMessagePromptTemplate.from_template(prompts["query_user"]),
        ]
    )

    # –ü—Ä–æ–º–ø—Ç –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–±–µ–∑ index, —Ç.–∫. –æ–Ω –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
    document_prompt = PromptTemplate(template="{page_content}", input_variables=["page_content"])

    # –°–æ–∑–¥–∞—ë–º –∫–∞—Å—Ç–æ–º–Ω—ã–π document separator —Å –Ω—É–º–µ—Ä–∞—Ü–∏–µ–π
    document_separator = "\n\n"

    # –°–æ–∑–¥–∞—Ç—å —Ü–µ–ø–æ—á–∫—É RetrievalQAWithSourcesChain —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    chain = RetrievalQAWithSourcesChain.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={
            "prompt": combine_prompt,
            "document_prompt": document_prompt,
            "document_variable_name": "summaries",
            "document_separator": document_separator,
        },
        verbose=False,
    )

    return chain


def parse_think_and_answer(text: str) -> Tuple[Optional[str], str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç <think> —Å–µ–∫—Ü–∏—é –∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏.

    Args:
        text: –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏

    Returns:
        (think_content, final_answer): –∫–æ—Ä—Ç–µ–∂ –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ <think> (–∏–ª–∏ None) –∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
    """
    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è <think>...</think>
    think_pattern = r"<think>(.*?)</think>"
    match = re.search(think_pattern, text, re.DOTALL)

    if match:
        think_content = match.group(1).strip()
        # –£–¥–∞–ª—è–µ–º <think> —Å–µ–∫—Ü–∏—é –∏–∑ —Ç–µ–∫—Å—Ç–∞, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        final_answer = re.sub(think_pattern, "", text, flags=re.DOTALL).strip()
        return think_content, final_answer
    else:
        # –ù–µ—Ç <think> —Å–µ–∫—Ü–∏–∏
        return None, text.strip()


def format_source_documents(source_documents: List[Document]) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫.

    Args:
        source_documents: —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

    Returns:
        –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —á–∞–Ω–∫–∞—Ö
    """
    if not source_documents:
        return ""

    lines = ["–ù–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏:"]
    for i, doc in enumerate(source_documents, 1):
        source = doc.metadata.get("source", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
        token_range = doc.metadata.get("token_range", "N/A")
        char_range = doc.metadata.get("char_range", "N/A")
        snippet: str = doc.page_content[:150].replace("\n", " ")
        lines.append(f"  [{i}] –ò—Å—Ç–æ—á–Ω–∏–∫: {source}")
        lines.append(f"      –¢–æ–∫–µ–Ω—ã: {token_range}, –°–∏–º–≤–æ–ª—ã: {char_range}")
        lines.append(f"      {snippet}...")

    return "\n".join(lines)


def answer_question(chain: RetrievalQAWithSourcesChain, question: str) -> Dict[str, Any]:
    """
    –û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –∏—Å–ø–æ–ª—å–∑—É—è RAG chain

    Args:
        chain: RAG chain —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º —Ä–µ—Ç—Ä–∏–≤–µ—Ä–æ–º
        question: –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    Returns:
        —Å–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º, –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–º–∏ think/answer —Å–µ–∫—Ü–∏—è–º–∏
    """
    logger.info("\n" + "=" * 80)
    logger.info(f"–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: {question}")
    logger.info("=" * 80)

    # –ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç chain (–ø—Ä–æ–º–ø—Ç –±—É–¥–µ—Ç –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω —á–µ—Ä–µ–∑ callback)
    result = chain.invoke({"question": question})

    # –†–∞–∑–¥–µ–ª–∏—Ç—å think —Å–µ–∫—Ü–∏—é –∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
    raw_answer = result.get("answer", "")
    think_content, final_answer = parse_think_and_answer(raw_answer)

    # –î–æ–±–∞–≤–∏—Ç—å –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
    result["think"] = think_content
    result["final_answer"] = final_answer

    return result


def main() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""

    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —á–∞—Ç-–±–æ—Ç–∞ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º —Ä–µ—Ç—Ä–∏–≤–µ—Ä–æ–º...")
    chain = create_rag_chain(use_chunk_filtering=True)
    print("‚úì –ì–æ—Ç–æ–≤–æ!\n")

    print("RAG —á–∞—Ç-–±–æ—Ç –≥–æ—Ç–æ–≤! (–≤–≤–µ–¥–∏—Ç–µ 'exit' –∏–ª–∏ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è)")
    print("–ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ\n")

    while True:
        question: str = input(">> –í–æ–ø—Ä–æ—Å: ").strip()

        if question.lower() in ("exit", "quit", "–≤—ã—Ö–æ–¥"):
            print("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break

        if not question:
            continue

        # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ RAG chain
        result = answer_question(chain, question)

        # –ü–æ–∫–∞–∑–∞—Ç—å think-—Å–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –µ—Å—Ç—å
        if result.get("think"):
            print(f"\nüí≠ Think-—Å–µ–∫—Ü–∏—è:")
            print(f"{result['think']}\n")

        # –ü–æ–∫–∞–∑–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        print(f"‚úÖ –û—Ç–≤–µ—Ç: {result.get('final_answer', '–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞')}\n")

        # –ü–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        if result.get("sources"):
            print(f"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {result['sources']}\n")

        # –ü–æ–∫–∞–∑–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        if result.get("source_documents"):
            chunks_text = format_source_documents(result["source_documents"])
            print(chunks_text)
            print()

        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º stdout –∏ –¥–∞—ë–º –≤—Ä–µ–º—è stderr –ª–æ–≥–∞–º –≤—ã–≤–µ—Å—Ç–∏—Å—å –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º prompt'–æ–º
        sys.stdout.flush()
        sys.stderr.flush()


if __name__ == "__main__":
    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s", datefmt="%H:%M:%S")
    main()
